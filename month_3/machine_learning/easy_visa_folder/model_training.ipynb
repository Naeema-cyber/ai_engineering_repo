{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a65d006",
   "metadata": {},
   "source": [
    "## **Phase 4: Model Training and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65032bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing our librarbies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e792d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed_easy_visa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ba46d",
   "metadata": {},
   "source": [
    "#### **Data Splitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28723b56",
   "metadata": {},
   "source": [
    "we will use stratified splitting based on the class imbalance of our targrt data which is the case status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e7d130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing selected features from  preprocessing \n",
    "\n",
    "selected_features = ['company_age', 'wage_per_year', 'wage_per_employee_ratio', 'employees_growth_rate_ratio', 'wage_per_age_ratio', 'case_status_encoded', 'education_level_ordinal', 'establishment_period_ordinal', 'continent_Africa', 'continent_Asia', 'continent_Europe', 'continent_North America', 'continent_Oceania', 'continent_South America', 'region_target_encoded', 'has_job_experience_encoded', 'requires_job_training_encoded', 'full_time_position_encoded', 'prevailing_wage_log', 'no_of_employees_log']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9179459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90d6832e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STRATIFIED DATA SPLITTING ===\n",
      "EDA identified class imbalance - using stratified splitting to preserve class distribution\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['case_status_encoded'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEDA identified class imbalance - using stratified splitting to preserve class distribution\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Select the chosen features\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m X_selected = \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSelected features shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_selected.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# First split: 70% train+val, 30% test\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ncc45554\\Desktop\\ai_engineering_repo\\my_venv\\Lib\\site-packages\\pandas\\core\\frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ncc45554\\Desktop\\ai_engineering_repo\\my_venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ncc45554\\Desktop\\ai_engineering_repo\\my_venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['case_status_encoded'] not in index\""
     ]
    }
   ],
   "source": [
    "\n",
    "X= df.drop('case_status_encoded', axis=1)\n",
    "y = df['case_status_encoded']\n",
    "\n",
    "\n",
    "# Stratified data splitting based on EDA findings about class imbalance\n",
    "print(\"=== STRATIFIED DATA SPLITTING ===\")\n",
    "print(\"EDA identified class imbalance - using stratified splitting to preserve class distribution\")\n",
    "\n",
    "# Select the chosen features\n",
    "X_selected = X[selected_features]\n",
    "print(f\"Selected features shape: {X_selected.shape}\")\n",
    "\n",
    "# First split: 70% train+val, 30% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: 75% train, 25% validation (of the 80%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nData split results:\")\n",
    "print(f\"Training set: {X_train.shape} ({(X_train.shape[0]/len(X_selected))*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape} ({(X_val.shape[0]/len(X_selected))*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape} ({(X_test.shape[0]/len(X_selected))*100:.1f}%)\")\n",
    "\n",
    "# Check class distribution in each set (should be similar due to stratification)\n",
    "print(f\"\\nClass distribution verification:\")\n",
    "print(\"Training set quality distribution:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(\"\\nValidation set quality distribution:\")\n",
    "print(y_val.value_counts().sort_index())\n",
    "print(\"\\nTest set quality distribution:\")\n",
    "print(y_test.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d60cd",
   "metadata": {},
   "source": [
    "#### **Feature scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50100cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE SCALING (STANDARD SCALER) ===\n",
      "EDA recommended StandardScaler for distance-based models\n",
      "✓ Scaling applied successfully!\n",
      "Training set scaled - Mean: -0.0000, Std: 1.0000\n",
      "Validation set scaled - Mean: -0.0004, Std: 1.0142\n",
      "Test set scaled - Mean: 0.0005, Std: 1.0045\n",
      "\n",
      "Scaling verification:\n",
      "Training set - Mean ≈ 0: True\n",
      "Training set - Std ≈ 1: True\n",
      "Validation set - Mean ≈ 0: True\n",
      "Validation set - Std ≈ 1: False\n",
      "Test set - Mean ≈ 0: True\n",
      "Test set - Std ≈ 1: True\n"
     ]
    }
   ],
   "source": [
    "# Using standard scaler as the data has little to no outliers based on the EDA recommendation\n",
    "print(\"=== FEATURE SCALING (STANDARD SCALER) ===\")\n",
    "print(\"EDA recommended StandardScaler for distance-based models\")\n",
    "# Fit scaler on training data only (to avoid data leakage)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "# Transform validation and test sets\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "print(\"✓ Scaling applied successfully!\")\n",
    "print(f\"Training set scaled - Mean: {X_train_scaled.mean().mean():.4f}, Std: {X_train_scaled.std().mean():.4f}\")\n",
    "print(f\"Validation set scaled - Mean: {X_val_scaled.mean().mean():.4f}, Std: {X_val_scaled.std().mean():.4f}\")\n",
    "print(f\"Test set scaled - Mean: {X_test_scaled.mean().mean():.4f}, Std: {X_test_scaled.std().mean():.4f}\")\n",
    "# Verify scaling worked correctly\n",
    "print(f\"\\nScaling verification:\")\n",
    "print(f\"Training set - Mean ≈ 0: {abs(X_train_scaled.mean().mean()) < 0.01}\")\n",
    "print(f\"Training set - Std ≈ 1: {abs(X_train_scaled.std().mean() - 1) < 0.01}\")\n",
    "print(f\"Validation set - Mean ≈ 0: {abs(X_val_scaled.mean().mean()) < 0.01}\")\n",
    "print(f\"Validation set - Std ≈ 1: {abs(X_val_scaled.std().mean() - 1) < 0.01}\")\n",
    "print(f\"Test set - Mean ≈ 0: {abs(X_test_scaled.mean().mean()) < 0.01}\")\n",
    "print(f\"Test set - Std ≈ 1: {abs(X_test_scaled.std().mean() - 1) < 0.01}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474fa19a",
   "metadata": {},
   "source": [
    "### **Algorithm selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65466c69",
   "metadata": {},
   "source": [
    "Task 4.2: Choose and justify the selection of machine learning algorithms (e.g., Linear Regression, Decision Tree, Random Forest, Gradient Boosting).\n",
    "\n",
    "Based on the EDA insights and recommendations, we would go with:\n",
    "- Gradient Boosting Classifier, XG Boost Classifier and LightGBM:\n",
    "    - *Dataset*: Our data is mostly categorical including the target and it is a large dataset containing above 10,000 samples.\n",
    "    - *Performance*: Gradient Boosting and XGBoost is best for tabular data like risk prediction and credit scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b7def",
   "metadata": {},
   "source": [
    "### **Model Comparison and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7384676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GradientBoost Model\n",
    "gb_clf = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "# Train the model\n",
    "gb_clf.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_train_pred_gb = gb_clf.predict(X_train)\n",
    "y_test_pred_gb = gb_clf.predict(X_test)\n",
    "# Evaluate performance\n",
    "train_accuracy_gb = accuracy_score(y_train, y_train_pred_gb)\n",
    "test_accuracy_gb = accuracy_score(y_test, y_test_pred_gb)\n",
    "train_balanced_acc_gb = balanced_accuracy_score(y_train, y_train_pred_gb)\n",
    "test_balanced_acc_gb = balanced_accuracy_score(y_test, y_test_pred_gb)\n",
    "train_f1_gb = f1_score(y_train, y_train_pred_gb, average='macro')\n",
    "test_f1_gb = f1_score(y_test, y_test_pred_gb, average='macro')\n",
    "print(f\"\\nXGBoost Performance:\")\n",
    "print(f\"Test - Accuracy: {test_accuracy_gb:.3f}, Balanced Acc: {test_balanced_acc_gb:.3f}, Macro F1: {test_f1_gb:.3f}\")\n",
    "# Feature importance analysis\n",
    "print(f\"\\nFeature Importance (Top 10):\")\n",
    "feature_importance_gb = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': gb_clf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "for i, (_, row) in enumerate(feature_importance_gb.head(10).iterrows(), 1):\n",
    "    print(f\"{i:2d}. {row['feature']}: {row['importance']:.3f}\")\n",
    "# Store XGBoost results\n",
    "xgb_results = {\n",
    "    'model': 'GradientBoosting',\n",
    "    'train_accuracy': train_accuracy_gb,\n",
    "    'test_accuracy': test_accuracy_gb,\n",
    "    'train_balanced_acc': train_balanced_acc_gb,\n",
    "    'test_balanced_acc': test_balanced_acc_gb,\n",
    "    'train_f1': train_f1_gb,\n",
    "    'test_f1': test_f1_gb\n",
    "}\n",
    "print(\"GradientBoost model completed!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
